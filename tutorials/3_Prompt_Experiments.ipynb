{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Experiments\n",
    "\n",
    "In this notebook we will experiment with prompts and the OpenAI models. We will try introducing different concepts and prompts using the very capable GPT-4 family of models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..\\\\code')\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from PIL import Image\n",
    "from doc_utils import *\n",
    "\n",
    "\n",
    "def show_img(img_path, width = None):\n",
    "    if width is not None:\n",
    "        display(HTML(f'<img src=\"{img_path}\" width={width}>'))\n",
    "    else:\n",
    "        display(Image.open(img_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure we have the OpenAI Models information\n",
    "\n",
    "We will need the GPT-4-Turbo and GPT-4-Vision models for this notebook.\n",
    "\n",
    "When running the below cell, the values should reflect the OpenAI reource you have created in the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {\n",
    "        'AZURE_OPENAI_RESOURCE': os.environ.get('AZURE_OPENAI_RESOURCE'),\n",
    "        'AZURE_OPENAI_KEY': os.environ.get('AZURE_OPENAI_KEY'),\n",
    "        'AZURE_OPENAI_MODEL_VISION': os.environ.get('AZURE_OPENAI_MODEL_VISION'),\n",
    "        'AZURE_OPENAI_MODEL': os.environ.get('AZURE_OPENAI_MODEL'),\n",
    "}\n",
    "\n",
    "model_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Sample Data\n",
    "\n",
    "Generate the sample images that will be used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF File 1_London_Brochure.pdf has 2 pages.\n",
      "Page 0 saved as sample_data/pdf_outputs/page_0.png\n",
      "Page 1 saved as sample_data/pdf_outputs/page_1.png\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Create a directory to store the outputs\n",
    "work_dir = \"sample_data/pdf_outputs\"\n",
    "os.makedirs(work_dir, exist_ok=True)\n",
    "\n",
    "# Load a sample PDF document\n",
    "\n",
    "def read_pdf(pdf_doc):\n",
    "    doc = fitz.open(pdf_doc)\n",
    "    print(f\"PDF File {os.path.basename(pdf_doc)} has {len(doc)} pages.\")\n",
    "    return doc\n",
    "\n",
    "def nb_extract_pages_as_png_files(doc):\n",
    "    png_files = []\n",
    "    for page in doc:\n",
    "        page_num = page.number\n",
    "        img_path = f\"{work_dir}/page_{page_num}.png\"\n",
    "        page_pix = page.get_pixmap(dpi=300)\n",
    "        page_pix.save(img_path)\n",
    "        print(f\"Page {page_num} saved as {img_path}\")\n",
    "        png_files.append(img_path)\n",
    "    \n",
    "    return png_files\n",
    "\n",
    "\n",
    "pdf_doc = \"sample_data/1_London_Brochure.pdf\"\n",
    "doc = read_pdf(pdf_doc)\n",
    "png_files = nb_extract_pages_as_png_files(doc)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Element Detection\n",
    "\n",
    "The first function we will experiment with is Detection. Sometimes, we need to detect whether a page has an image or embedded inside it, or a table, using a Vision model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table Detection\n",
    "\n",
    "In the below cell, we are trying to detect tables in the png files generated from the PDF document. The code below will display as an image the pages where tables were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16.03.2024_16.07.52 :: \u001b[92mStart of GPT4V Call to process file(s) ['sample_data/pdf_outputs/page_0.png'] with model: https://oai-tst-sweden.openai.azure.com/ \u001b[0m\n",
      "endpoint https://oai-tst-sweden.openai.azure.com/openai/deployments/gpt4v/extensions/chat/completions?api-version=2023-12-01-preview\n",
      "\n",
      "16.03.2024_16.07.58 :: \u001b[92mEnd of GPT4V Call to process file(s) ['sample_data/pdf_outputs/page_0.png'] with model: https://oai-tst-sweden.openai.azure.com/ \u001b[0m\n",
      "Status: Image was successfully explained, with Status Code: 200\n",
      "Result: 0 tables detected in the PDF page.\n",
      "\n",
      "16.03.2024_16.07.58 :: \u001b[92mStart of GPT4V Call to process file(s) ['sample_data/pdf_outputs/page_1.png'] with model: https://oai-tst-sweden.openai.azure.com/ \u001b[0m\n",
      "endpoint https://oai-tst-sweden.openai.azure.com/openai/deployments/gpt4v/extensions/chat/completions?api-version=2023-12-01-preview\n",
      "\n",
      "16.03.2024_16.08.04 :: \u001b[92mEnd of GPT4V Call to process file(s) ['sample_data/pdf_outputs/page_1.png'] with model: https://oai-tst-sweden.openai.azure.com/ \u001b[0m\n",
      "Status: Image was successfully explained, with Status Code: 200\n",
      "Result: 1 tables detected in the PDF page.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"sample_data/pdf_outputs/page_1.png\" width=400>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "detect_num_of_tables_prompt = \"\"\"\n",
    "You are an assistant working on a document processing task that involves detecting and counting the number of data tables in am image file using a vision model. Given an image, your task is determine the number of data tables present. \n",
    "\n",
    "Output:\n",
    "Return a single integer representing the number of data tables detected in the page. Do **NOT** generate any other text or explanation, just the number of tables. We are **NOT** looking for the word 'table' in the text, we are looking for the number of data tables in the image.\n",
    "\n",
    "\"\"\"\n",
    "for png in png_files:\n",
    "    result, description = call_gpt4v(png, gpt4v_prompt = detect_num_of_tables_prompt, temperature = 0.2, model_info=model_info)\n",
    "    print(f\"Status: {description}\")\n",
    "    print(f\"Result: {result} tables detected in the PDF page.\")\n",
    "\n",
    "    if int(result) > 0:\n",
    "        show_img(png, width=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Detection\n",
    "\n",
    "In the below cell, we are trying to detect images in the png files generated from the PDF document. The code below will display as an image the pages where images were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16.03.2024_16.08.44 :: \u001b[92mStart of GPT4V Call to process file(s) ['sample_data/pdf_outputs/page_0.png'] with model: https://oai-tst-sweden.openai.azure.com/ \u001b[0m\n",
      "endpoint https://oai-tst-sweden.openai.azure.com/openai/deployments/gpt4v/extensions/chat/completions?api-version=2023-12-01-preview\n",
      "\n",
      "16.03.2024_16.08.50 :: \u001b[92mEnd of GPT4V Call to process file(s) ['sample_data/pdf_outputs/page_0.png'] with model: https://oai-tst-sweden.openai.azure.com/ \u001b[0m\n",
      "Status: Image was successfully explained, with Status Code: 200\n",
      "Result: 2 images detected in the PDF page.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"sample_data/pdf_outputs/page_0.png\" width=400>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16.03.2024_16.08.50 :: \u001b[92mStart of GPT4V Call to process file(s) ['sample_data/pdf_outputs/page_1.png'] with model: https://oai-tst-sweden.openai.azure.com/ \u001b[0m\n",
      "endpoint https://oai-tst-sweden.openai.azure.com/openai/deployments/gpt4v/extensions/chat/completions?api-version=2023-12-01-preview\n",
      "\n",
      "16.03.2024_16.08.55 :: \u001b[92mEnd of GPT4V Call to process file(s) ['sample_data/pdf_outputs/page_1.png'] with model: https://oai-tst-sweden.openai.azure.com/ \u001b[0m\n",
      "Status: Image was successfully explained, with Status Code: 200\n",
      "Result: 0 images detected in the PDF page.\n"
     ]
    }
   ],
   "source": [
    "detect_num_of_diagrams_prompt = \"\"\"\n",
    "You are an assistant working on a document processing task that involves detecting and counting the number of visual assets in a document page using a vision model. Given a screenshot of a documenat page, your task is determine the number of visual assets present. Please ignore any standard non-visual assets such as text, headers, footers, page numbers, tables, etc.\n",
    "\n",
    "What is meant by visual assets: infographics, maps, flowcharts, timelines, tables, illustrations, icons, heatmaps, scatter plots, pie charts, bar graphs, line graphs, histograms, Venn diagrams, organizational charts, mind maps, Gantt charts, tree diagrams, pictograms, schematics, blueprints, 3D models, storyboards, wireframes, dashboards, comic strips, story maps, process diagrams, network diagrams, bubble charts, area charts, radar charts, waterfall charts, funnel charts, sunburst charts, sankey diagrams, choropleth maps, isometric drawings, exploded views, photomontages, collages, mood boards, concept maps, fishbone diagrams, decision trees, Pareto charts, control charts, spider charts, images, diagrams, logos, charts or graphs.\n",
    "\n",
    "Output:\n",
    "Return a single integer representing the number of visual assets detected in the page. Do **NOT** generate any other text or explanation, just the count of . \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for png in png_files:\n",
    "    result, description = call_gpt4v(png, gpt4v_prompt = detect_num_of_diagrams_prompt, temperature = 0.2, model_info=model_info)\n",
    "    print(f\"Status: {description}\")\n",
    "    print(f\"Result: {result} images detected in the PDF page.\")\n",
    "\n",
    "    if int(result) > 0:\n",
    "        show_img(png, width=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Code\n",
    "\n",
    "Read in the doc_utils.py library, and generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- `ingest_docs_directory`\n",
      "  - `ingest_doc`\n",
      "    - `process_pdf`\n",
      "      - `extract_high_res_page_images`\n",
      "      - `extract_text`\n",
      "      - `harvest_code`\n",
      "      - `extract_images`\n",
      "      - `post_process_images`\n",
      "      - `extract_tables`\n",
      "      - `post_process_tables`\n",
      "    - `commit_assets_to_vector_index`\n",
      "      - `add_asset_to_vec_store`\n",
      "        - `create_metadata`\n",
      "        - `get_embeddings`\n",
      "        - `get_solution_path_components`\n",
      "        - `generate_uuid_from_string`\n",
      "    - `save_docx_as_pdf`\n",
      "    - `save_pptx_as_pdf`\n",
      "\n",
      "```mermaid\n",
      "graph TD;\n",
      "    ingest_docs_directory --> ingest_doc;\n",
      "    ingest_doc --> process_pdf;\n",
      "    process_pdf --> extract_high_res_page_images;\n",
      "    process_pdf --> extract_text;\n",
      "    process_pdf --> harvest_code;\n",
      "    process_pdf --> extract_images;\n",
      "    process_pdf --> post_process_images;\n",
      "    process_pdf --> extract_tables;\n",
      "    process_pdf --> post_process_tables;\n",
      "    ingest_doc --> commit_assets_to_vector_index;\n",
      "    commit_assets_to_vector_index --> add_asset_to_vec_store;\n",
      "    add_asset_to_vec_store --> create_metadata;\n",
      "    add_asset_to_vec_store --> get_embeddings;\n",
      "    add_asset_to_vec_store --> get_solution_path_components;\n",
      "    add_asset_to_vec_store --> generate_uuid_from_string;\n",
      "    ingest_doc --> save_docx_as_pdf;\n",
      "    ingest_doc --> save_pptx_as_pdf;\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "{code}\n",
    "\n",
    "In the library above, detect the important functions in the '{func_name}' tree. Start with the '{func_name}' function, and then list the essential functions called by that '{func_name}'. Please ignore small functions that are 3 lines of code or less. Focus only on the custom functions defined in the code using the keyword 'def', and ignore the imported functions or system functions like fitz.open and glob.glob. Then please do the following:\n",
    "\n",
    "    1. Output the list in bullet-point format\n",
    "    2. Show their relationship by generating Mermaid code that represents these functions.\n",
    "    3. Make sure the relationships are correct. The arrow should point from the calling function to the called function.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "p = prompt.format(code = read_asset_file(\"../code/doc_utils.py\")[0], func_name = 'ingest_docs_directory')\n",
    "\n",
    "output = ask_LLM(p, model_info=model_info)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render the Mermaid code\n",
    "\n",
    "To render the above generated Mermaid Code, please copy the above Mermaid script block, visit [mermaid.live](https://mermaid.live) in your browser, and paste the copied script in your browser. \n",
    "\n",
    "The image should be rendered immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Mermaid Representation](../images/ingestion_tree.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
