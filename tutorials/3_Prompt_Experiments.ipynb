{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Experiments\n",
    "\n",
    "In this notebook we will experiment with prompts and the OpenAI models. We will try introducing different concepts and prompts using the very capable GPT-4 family of models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..\\\\code')\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from PIL import Image\n",
    "from doc_utils import *\n",
    "\n",
    "\n",
    "def show_img(img_path, width = None):\n",
    "    if width is not None:\n",
    "        display(HTML(f'<img src=\"{img_path}\" width={width}>'))\n",
    "    else:\n",
    "        display(Image.open(img_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure we have the OpenAI Models information\n",
    "\n",
    "We will need the GPT-4-Turbo and GPT-4-Vision models for this notebook.\n",
    "\n",
    "When running the below cell, the values should reflect the OpenAI reource you have created in the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {\n",
    "        'AZURE_OPENAI_RESOURCE': os.environ.get('AZURE_OPENAI_RESOURCE'),\n",
    "        'AZURE_OPENAI_KEY': os.environ.get('AZURE_OPENAI_KEY'),\n",
    "        'AZURE_OPENAI_MODEL_VISION': os.environ.get('AZURE_OPENAI_MODEL_VISION'),\n",
    "        'AZURE_OPENAI_MODEL': os.environ.get('AZURE_OPENAI_MODEL'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Sample Data\n",
    "\n",
    "Generate the sample images that will be used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF File 1_London_Brochure.pdf has 2 pages.\n",
      "Page 0 saved as sample_data/pdf_outputs/page_0.png\n",
      "Page 1 saved as sample_data/pdf_outputs/page_1.png\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Create a directory to store the outputs\n",
    "work_dir = \"sample_data/pdf_outputs\"\n",
    "os.makedirs(work_dir, exist_ok=True)\n",
    "\n",
    "# Load a sample PDF document\n",
    "\n",
    "def read_pdf(pdf_doc):\n",
    "    doc = fitz.open(pdf_doc)\n",
    "    print(f\"PDF File {os.path.basename(pdf_doc)} has {len(doc)} pages.\")\n",
    "    return doc\n",
    "\n",
    "def nb_extract_pages_as_png_files(doc):\n",
    "    png_files = []\n",
    "    for page in doc:\n",
    "        page_num = page.number\n",
    "        img_path = f\"{work_dir}/page_{page_num}.png\"\n",
    "        page_pix = page.get_pixmap(dpi=300)\n",
    "        page_pix.save(img_path)\n",
    "        print(f\"Page {page_num} saved as {img_path}\")\n",
    "        png_files.append(img_path)\n",
    "    \n",
    "    return png_files\n",
    "\n",
    "\n",
    "pdf_doc = \"sample_data/1_London_Brochure.pdf\"\n",
    "doc = read_pdf(pdf_doc)\n",
    "png_files = nb_extract_pages_as_png_files(doc)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Element Detection\n",
    "\n",
    "The first function we will experiment with is Detection. Sometimes, we need to detect whether a page has an image or embedded inside it, or a table, using a Vision model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table Detection\n",
    "\n",
    "In the below cell, we are trying to detect tables in the png files generated from the PDF document. The code below will display as an image the pages where tables were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16.03.2024_16.07.52 :: \u001b[92mStart of GPT4V Call to process file(s) ['sample_data/pdf_outputs/page_0.png'] with model: https://oai-tst-sweden.openai.azure.com/ \u001b[0m\n",
      "endpoint https://oai-tst-sweden.openai.azure.com/openai/deployments/gpt4v/extensions/chat/completions?api-version=2023-12-01-preview\n",
      "\n",
      "16.03.2024_16.07.58 :: \u001b[92mEnd of GPT4V Call to process file(s) ['sample_data/pdf_outputs/page_0.png'] with model: https://oai-tst-sweden.openai.azure.com/ \u001b[0m\n",
      "Status: Image was successfully explained, with Status Code: 200\n",
      "Result: 0 tables detected in the PDF page.\n",
      "\n",
      "16.03.2024_16.07.58 :: \u001b[92mStart of GPT4V Call to process file(s) ['sample_data/pdf_outputs/page_1.png'] with model: https://oai-tst-sweden.openai.azure.com/ \u001b[0m\n",
      "endpoint https://oai-tst-sweden.openai.azure.com/openai/deployments/gpt4v/extensions/chat/completions?api-version=2023-12-01-preview\n",
      "\n",
      "16.03.2024_16.08.04 :: \u001b[92mEnd of GPT4V Call to process file(s) ['sample_data/pdf_outputs/page_1.png'] with model: https://oai-tst-sweden.openai.azure.com/ \u001b[0m\n",
      "Status: Image was successfully explained, with Status Code: 200\n",
      "Result: 1 tables detected in the PDF page.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"sample_data/pdf_outputs/page_1.png\" width=400>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "detect_num_of_tables_prompt = \"\"\"\n",
    "You are an assistant working on a document processing task that involves detecting and counting the number of data tables in am image file using a vision model. Given an image, your task is determine the number of data tables present. \n",
    "\n",
    "Output:\n",
    "Return a single integer representing the number of data tables detected in the page. Do **NOT** generate any other text or explanation, just the number of tables. We are **NOT** looking for the word 'table' in the text, we are looking for the number of data tables in the image.\n",
    "\n",
    "\"\"\"\n",
    "for png in png_files:\n",
    "    result, description = call_gpt4v(png, gpt4v_prompt = detect_num_of_tables_prompt, temperature = 0.2, model_info=model_info)\n",
    "    print(f\"Status: {description}\")\n",
    "    print(f\"Result: {result} tables detected in the PDF page.\")\n",
    "\n",
    "    if int(result) > 0:\n",
    "        show_img(png, width=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Detection\n",
    "\n",
    "In the below cell, we are trying to detect images in the png files generated from the PDF document. The code below will display as an image the pages where images were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16.03.2024_16.08.44 :: \u001b[92mStart of GPT4V Call to process file(s) ['sample_data/pdf_outputs/page_0.png'] with model: https://oai-tst-sweden.openai.azure.com/ \u001b[0m\n",
      "endpoint https://oai-tst-sweden.openai.azure.com/openai/deployments/gpt4v/extensions/chat/completions?api-version=2023-12-01-preview\n",
      "\n",
      "16.03.2024_16.08.50 :: \u001b[92mEnd of GPT4V Call to process file(s) ['sample_data/pdf_outputs/page_0.png'] with model: https://oai-tst-sweden.openai.azure.com/ \u001b[0m\n",
      "Status: Image was successfully explained, with Status Code: 200\n",
      "Result: 2 images detected in the PDF page.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"sample_data/pdf_outputs/page_0.png\" width=400>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16.03.2024_16.08.50 :: \u001b[92mStart of GPT4V Call to process file(s) ['sample_data/pdf_outputs/page_1.png'] with model: https://oai-tst-sweden.openai.azure.com/ \u001b[0m\n",
      "endpoint https://oai-tst-sweden.openai.azure.com/openai/deployments/gpt4v/extensions/chat/completions?api-version=2023-12-01-preview\n",
      "\n",
      "16.03.2024_16.08.55 :: \u001b[92mEnd of GPT4V Call to process file(s) ['sample_data/pdf_outputs/page_1.png'] with model: https://oai-tst-sweden.openai.azure.com/ \u001b[0m\n",
      "Status: Image was successfully explained, with Status Code: 200\n",
      "Result: 0 images detected in the PDF page.\n"
     ]
    }
   ],
   "source": [
    "detect_num_of_diagrams_prompt = \"\"\"\n",
    "You are an assistant working on a document processing task that involves detecting and counting the number of visual assets in a document page using a vision model. Given a screenshot of a documenat page, your task is determine the number of visual assets present. Please ignore any standard non-visual assets such as text, headers, footers, page numbers, tables, etc.\n",
    "\n",
    "What is meant by visual assets: infographics, maps, flowcharts, timelines, tables, illustrations, icons, heatmaps, scatter plots, pie charts, bar graphs, line graphs, histograms, Venn diagrams, organizational charts, mind maps, Gantt charts, tree diagrams, pictograms, schematics, blueprints, 3D models, storyboards, wireframes, dashboards, comic strips, story maps, process diagrams, network diagrams, bubble charts, area charts, radar charts, waterfall charts, funnel charts, sunburst charts, sankey diagrams, choropleth maps, isometric drawings, exploded views, photomontages, collages, mood boards, concept maps, fishbone diagrams, decision trees, Pareto charts, control charts, spider charts, images, diagrams, logos, charts or graphs.\n",
    "\n",
    "Output:\n",
    "Return a single integer representing the number of visual assets detected in the page. Do **NOT** generate any other text or explanation, just the count of . \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for png in png_files:\n",
    "    result, description = call_gpt4v(png, gpt4v_prompt = detect_num_of_diagrams_prompt, temperature = 0.2, model_info=model_info)\n",
    "    print(f\"Status: {description}\")\n",
    "    print(f\"Result: {result} images detected in the PDF page.\")\n",
    "\n",
    "    if int(result) > 0:\n",
    "        show_img(png, width=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Code\n",
    "\n",
    "Read in the doc_utils.py library, and generate the function tree for the `ingest_docs_directory` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- `ingest_docs_directory`\n",
      "  - `ingest_doc`\n",
      "    - `process_pdf`\n",
      "      - `extract_high_res_page_images`\n",
      "      - `extract_text`\n",
      "      - `harvest_code`\n",
      "      - `extract_images`\n",
      "      - `post_process_images`\n",
      "      - `extract_tables`\n",
      "      - `post_process_tables`\n",
      "    - `commit_assets_to_vector_index`\n",
      "      - `add_asset_to_vec_store`\n",
      "        - `create_metadata`\n",
      "        - `get_embeddings`\n",
      "        - `get_solution_path_components`\n",
      "        - `generate_uuid_from_string`\n",
      "    - `save_docx_as_pdf`\n",
      "    - `save_pptx_as_pdf`\n",
      "\n",
      "```mermaid\n",
      "graph TD;\n",
      "    ingest_docs_directory --> ingest_doc;\n",
      "    ingest_doc --> process_pdf;\n",
      "    process_pdf --> extract_high_res_page_images;\n",
      "    process_pdf --> extract_text;\n",
      "    process_pdf --> harvest_code;\n",
      "    process_pdf --> extract_images;\n",
      "    process_pdf --> post_process_images;\n",
      "    process_pdf --> extract_tables;\n",
      "    process_pdf --> post_process_tables;\n",
      "    ingest_doc --> commit_assets_to_vector_index;\n",
      "    commit_assets_to_vector_index --> add_asset_to_vec_store;\n",
      "    add_asset_to_vec_store --> create_metadata;\n",
      "    add_asset_to_vec_store --> get_embeddings;\n",
      "    add_asset_to_vec_store --> get_solution_path_components;\n",
      "    add_asset_to_vec_store --> generate_uuid_from_string;\n",
      "    ingest_doc --> save_docx_as_pdf;\n",
      "    ingest_doc --> save_pptx_as_pdf;\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "{code}\n",
    "\n",
    "In the library above, detect the important functions in the '{func_name}' tree. Start with the '{func_name}' function, and then list the essential functions called by that '{func_name}'. Please ignore small functions that are 3 lines of code or less. Focus only on the custom functions defined in the code using the keyword 'def', and ignore the imported functions or system functions like fitz.open and glob.glob. Then please do the following:\n",
    "\n",
    "    1. Output the list in bullet-point format\n",
    "    2. Show their relationship by generating Mermaid code that represents these functions.\n",
    "    3. Make sure the relationships are correct. The arrow should point from the calling function to the called function.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "p = prompt.format(code = read_asset_file(\"../code/doc_utils.py\")[0], func_name = 'ingest_docs_directory')\n",
    "\n",
    "output = ask_LLM(p, model_info=model_info)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render the Mermaid code\n",
    "\n",
    "To render the above generated Mermaid Code, please copy the above Mermaid script block, visit [mermaid.live](https://mermaid.live) in your browser, and paste the copied script in your browser. \n",
    "\n",
    "The image should be rendered immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Mermaid Representation](../images/ingestion_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the `search` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- `search`\n",
      "  - `aggregate_ai_search`\n",
      "    - `get_query_entities`\n",
      "    - `call_ai_search`\n",
      "  - `generate_search_assets`\n",
      "  - `apply_computation_support`\n",
      "    - `try_code_interpreter_for_tables_using_taskweaver`\n",
      "    - `try_code_interpreter_for_tables_using_python_exec`\n",
      "    - `try_code_interpreter_for_tables_using_assistants_api`\n",
      "  - `clean_up_text`\n",
      "  - `generate_search_assets`\n",
      "  - `process_assistants_api_response`\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    search --> aggregate_ai_search\n",
      "    aggregate_ai_search --> get_query_entities\n",
      "    aggregate_ai_search --> call_ai_search\n",
      "    search --> generate_search_assets\n",
      "    search --> apply_computation_support\n",
      "    apply_computation_support --> try_code_interpreter_for_tables_using_taskweaver\n",
      "    apply_computation_support --> try_code_interpreter_for_tables_using_python_exec\n",
      "    apply_computation_support --> try_code_interpreter_for_tables_using_assistants_api\n",
      "    search --> clean_up_text\n",
      "    search --> generate_search_assets\n",
      "    search --> process_assistants_api_response\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "{code}\n",
    "\n",
    "In the library above, detect the important functions in the '{func_name}' tree. Start with the '{func_name}' function, and then list the essential functions called by that '{func_name}'. Please ignore small functions that are 3 lines of code or less. Focus only on the custom functions defined in the code using the keyword 'def', and ignore the imported functions or system functions like fitz.open and glob.glob. Then please do the following:\n",
    "\n",
    "    1. Output the list in bullet-point format\n",
    "    2. Show their relationship by generating Mermaid code that represents these functions.\n",
    "    3. Make sure the relationships are correct. The arrow should point from the calling function to the called function.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "p = prompt.format(code = read_asset_file(\"../code/doc_utils.py\")[0], func_name = 'search')\n",
    "\n",
    "output = ask_LLM(p, model_info=model_info)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And render the Mermaid code .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Mermaid Representation](../images/search_func_tree.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The `create_doc_chunks_with_doc_int_markdown` function is integral to the processing of documents, particularly when utilizing the Document Intelligence service. It's designed to handle the markdown conversion of document chunks, ensuring that the extracted data is formatted correctly for further analysis. This function is applicable to various document formats and is capable of processing text, images, and tables, making it versatile in the multimodal information extraction process. Its role is crucial in structuring the raw extracted data into a more accessible and analyzable form.\n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The `create_image_doc_chunks` function is integral to the processing of image data within multimodal documents. It specifically targets the extraction and organization of image-related content, segmenting each image as a discrete chunk for further analysis. This function is applicable across various document formats that include image data, playing a crucial role in the multimodal extraction pipeline by ensuring that visual information is accurately captured and prepared for subsequent processing steps such as tagging and analysis. It deals exclusively with the image modality, isolating it from text and tables to streamline the handling of visual content.\n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The `create_pdf_chunks` function is a crucial step in the document ingestion process, particularly for PDF files. It segments the input PDF document into individual chunks, which are then processed separately in subsequent stages of the pipeline. This function is applicable to all modalities within a PDF document, including text, images, and tables, ensuring a comprehensive breakdown of the document's content for detailed analysis and extraction. Its role is foundational, as it sets the stage for the specialized processing of each modality by other functions in the pipeline.\n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The function `create_table_doc_chunks_markdown` is responsible for processing tables within documents, specifically converting them into Markdown format. It is applicable to `.xlsx` files as part of the `openpyxl` pipeline. This function not only handles the conversion but also manages the chunking of tables when they are too large, ensuring that the Markdown representation is accurate and manageable. It processes the table modality exclusively and is crucial for preserving the structure and data of tables during the document ingestion process.\n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The `delete_pdf_chunks` function is a crucial step in the document processing pipeline, particularly for PDF files. It is responsible for removing the temporary storage of PDF chunks from memory, ensuring that the system resources are efficiently managed and not overburdened with unnecessary data. This function is applied after the initial extraction of high-resolution images and text from the PDF document, and before any post-processing of images or tables. It is applicable to all modalities—text, images, and tables—since it deals with the cleanup of data extracted from PDF chunks.\n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The `extract_doc_using_doc_int` function is a key component in the document processing pipeline, specifically tailored for handling `.docx` and `.pdf` files. It leverages the capabilities of Azure's Document Intelligence Service to analyze and extract structured data, including text and tables, from documents. This function is crucial for converting document content into a format that can be further processed for insights and is versatile in dealing with both textual and tabular data modalities.\n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The `extract_docx_using_py_docx` function is designed to handle the extraction of content from `.docx` files, specifically focusing on text, images, and tables. It utilizes the `python-docx` library to access and extract these elements, ensuring that the data is accurately retrieved and stored in a structured format suitable for further processing. This function is crucial for the initial stage of the ingestion pipeline, setting the foundation for subsequent analysis and processing steps. It is applicable to `.docx` files and is responsible for extracting all three modalities: text, images, and tables, from the document.\n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The `extract_tables_from_images` function is designed to identify and extract tables from image files within a document. It applies to image modalities, specifically targeting visual data representations such as tables embedded within image files. This function is crucial for converting visual table data into a structured format that can be further processed or analyzed, making it an essential step in multimodal document processing pipelines that deal with both textual and visual information. It is particularly relevant for documents where tabular information is presented in non-textual formats.\n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The `extract_xlsx_using_openpyxl` function is designed to handle the extraction of data from `.xlsx` files, specifically focusing on the retrieval of tables and their conversion into various formats for further processing. It leverages the `openpyxl` library to access and manipulate Excel files, ensuring that the extracted data is accurately represented in Python-friendly structures such as DataFrames. This function is crucial for parsing spreadsheet data, which is often rich in structured information, making it a key step in the data extraction phase for `.xlsx` files within the ingestion pipeline. It processes the table modality, transforming Excel sheets into Markdown, plain text, and Python scripts, which can then be integrated into the multimodal information extraction framework.\n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The `generate_analysis_for_text` function is designed to analyze the relationship between a specific text chunk and the overall content of a document. It highlights entity relationships introduced or extended in the text chunk, providing a concise analysis that adds context to the document's topics. This function is applicable to all modalities—text, images, and tables—ensuring a comprehensive understanding of the document's content. It plays a crucial role in enhancing the document's metadata by providing insights into the significance of each section within the larger document structure.\n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The `generate_document_wide_summary` function is responsible for creating a concise summary of the entire document's content. It extracts key information and presents it in a few paragraphs, ensuring that the essence of the document is captured without unnecessary details. This function is applicable to all document formats, including text, images, and tables, making it a versatile component in the multimodal information extraction pipeline. It plays a crucial role in providing a quick overview of the document, which can be beneficial for both indexing and search purposes.\n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The `generate_document_wide_tags` function is a crucial component in the document ingestion pipeline, applicable across various document formats including PDF, DOCX, and XLSX. It is responsible for extracting key tags from the entire document, which are essential for enhancing search and retrieval capabilities. This function processes text modality, ensuring that significant entities and topics within the document are captured as tags, aiding in the creation of a searchable index for the ingested content.\n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The `generate_tags_for_all_chunks` function is integral to the multimodal information extraction process, applicable across various document formats including PDF, DOCX, and XLSX. It operates on all three modalities—text, images, and tables—extracting and optimizing tags for enhanced search and retrieval within a vector store. This function ensures that each chunk of the document, regardless of its content type, is accurately represented by a set of descriptive tags, facilitating efficient indexing and subsequent search operations.\n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The `pdf_extract_high_res_chunk_images` function is responsible for extracting high-resolution images from each chunk of a PDF document. It plays a crucial role in the initial stages of the document processing pipeline, particularly for PDF formats, ensuring that visual data is captured in detail for subsequent analysis. This function focuses on the image modality, converting document chunks into PNG images at a DPI of 300, which are then used for further image-based processing tasks.\n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The `pdf_extract_images` function is designed to handle the extraction of images from PDF documents. It is applicable to PDF formats and operates within a multimodal extraction context, where it specifically processes the image modality. This function plays a crucial role in isolating visual content from PDFs, which is essential for subsequent image analysis and understanding in the broader multimodal information extraction process.\n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The `pdf_extract_text` function is a crucial component in the document processing pipeline, specifically tailored for handling PDF files. It is responsible for extracting textual content from each page of a PDF document, converting it into a machine-readable format. This function is pivotal for subsequent stages that may involve text analysis, search indexing, or further data extraction tasks. It operates solely on the text modality, ensuring that the rich textual information embedded within PDFs is accurately captured and made available for downstream processing.\n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The `post_process_images` function is integral to refining the output from image extraction operations within the document ingestion process. It specifically handles the enhancement and clarification of images, ensuring that any visual data is accurately represented and usable for subsequent analysis. This function is applicable across various document formats that include image content, playing a pivotal role in multimodal information extraction where visual data is a key component. It is designed to work with images as a modality, complementing other functions that handle text and tables.\n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4 - Endpoint: https://temp-aoai-tst-swc.openai.azure.com//openai/\n",
      "\n",
      "The `post_process_tables` function is designed to handle the refinement of table data extracted from documents. It applies to various document formats, including PDFs and images, where tables are present. The function's role is to enhance the quality of the extracted table information, ensuring that it is accurately represented and formatted for further use. It specifically deals with the 'table' modality, focusing on the post-extraction processing of tables to prepare them for integration into a searchable vector index or for analytical computations.\n"
     ]
    }
   ],
   "source": [
    "ss = \"create_doc_chunks_with_doc_int_markdown,create_image_doc_chunks,create_pdf_chunks,create_table_doc_chunks_markdown,delete_pdf_chunks,extract_doc_using_doc_int,extract_docx_using_py_docx,extract_tables_from_images,extract_xlsx_using_openpyxl,generate_analysis_for_text,generate_document_wide_summary,generate_document_wide_tags,generate_tags_for_all_chunks,pdf_extract_high_res_chunk_images,pdf_extract_images,pdf_extract_text,post_process_images,post_process_tables\"\n",
    "\n",
    "\n",
    "for s in ss.split(','):\n",
    "\n",
    "    prompt = \"\"\"You are a code analyzer assistant, and you help with generating documentation and code analysis.\n",
    "\n",
    "    ## START OF CODE\n",
    "    {code}\n",
    "    ## END OF CODE\n",
    "\n",
    "    ## START OF PIPELINES DEFINITIONS\n",
    "    {{\n",
    "        \".pdf\": {{\n",
    "            \"gpt-4-vision\": [\n",
    "                \"create_pdf_chunks\", \"pdf_extract_high_res_chunk_images\", \"pdf_extract_text\", \"pdf_extract_images\", \"delete_pdf_chunks\", \"post_process_images\", \"extract_tables_from_images\", \"post_process_tables\", \"generate_tags_for_all_chunks\", \"generate_document_wide_tags\", \"generate_document_wide_summary\", \"generate_analysis_for_text\"\n",
    "            ],\n",
    "            \"document-intelligence\": [\n",
    "                \"create_pdf_chunks\", \"pdf_extract_high_res_chunk_images\", \"pdf_extract_text\", \"pdf_extract_images\", \"delete_pdf_chunks\", \"extract_doc_using_doc_int\", \"create_doc_chunks_with_doc_int_markdown\", \"post_process_images\", \"generate_tags_for_all_chunks\", \"generate_document_wide_tags\", \"generate_document_wide_summary\", \"generate_analysis_for_text\"\n",
    "            ],\n",
    "            \"hybrid\": [\n",
    "                \"create_pdf_chunks\", \"pdf_extract_high_res_chunk_images\", \"delete_pdf_chunks\", \"extract_doc_using_doc_int\", \"create_doc_chunks_with_doc_int_markdown\", \"post_process_images\", \"post_process_tables\", \"generate_tags_for_all_chunks\", \"generate_document_wide_tags\", \"generate_document_wide_summary\", \"generate_analysis_for_text\"\n",
    "            ]\n",
    "        }},\n",
    "        \".docx\": {{\n",
    "            \"py-docx\": [\n",
    "                \"extract_docx_using_py_docx\", \"create_doc_chunks_with_doc_int_markdown\", \"post_process_images\", \"generate_tags_for_all_chunks\", \"generate_document_wide_tags\", \"generate_document_wide_summary\", \"generate_analysis_for_text\"\n",
    "            ],\n",
    "            \"document-intelligence\": [\n",
    "                \"extract_doc_using_doc_int\", \"create_doc_chunks_with_doc_int_markdown\", \"post_process_images\", \"generate_tags_for_all_chunks\", \"generate_document_wide_tags\", \"generate_document_wide_summary\", \"generate_analysis_for_text\"\n",
    "            ]\n",
    "        }},\n",
    "        \".xlsx\": {{\n",
    "            \"openpyxl\": [\n",
    "                \"extract_xlsx_using_openpyxl\", \"create_table_doc_chunks_markdown\", \"create_image_doc_chunks\", \"generate_tags_for_all_chunks\", \"generate_document_wide_tags\", \"generate_document_wide_summary\", \"generate_analysis_for_text\"\n",
    "            ]\n",
    "        }}\n",
    "    }}\n",
    "    ## END OF PIPELINES DEFINITIONS\n",
    "\n",
    "    In the library above, please generate a concise documentation about the important function '{func_name}'. This Python documentation needs to be concise in around 3 or 4 sentences. The function is part of a processing pipeline that is executed during document ingestion for multimodal information extraction. Some functions are only specific to certain formats as outlined by the Pipelines Definitions above. Do not mention that this function is part of a document ingestion pipeline in your final answer. Comment about the relevance and position of this function in the overall pipeline, and to which formats it applied. Some functions process either text, images or tables, and others process all 3 modalities (or a combination), so please make sure to mention explicitly which modalities this function processes. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    p = prompt.format(code = read_asset_file(\"../code/doc_utils.py\")[0], func_name = s)\n",
    "\n",
    "    output = ask_LLM(p, model_info=model_info)\n",
    "    print(output)\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
