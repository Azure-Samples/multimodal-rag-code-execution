{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markdown Table Chunking\n",
    "\n",
    "In this notebook we will experiment with Markdown table chunking using various approaches. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..\\\\code')\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from IPython.display import display, JSON, Markdown, HTML\n",
    "import copy\n",
    "from PIL import Image\n",
    "from doc_utils import *\n",
    "\n",
    "\n",
    "def show_img(img_path, width = None):\n",
    "    if width is not None:\n",
    "        display(HTML(f'<img src=\"{img_path}\" width={width}>'))\n",
    "    else:\n",
    "        display(Image.open(img_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure we have the OpenAI Models information\n",
    "\n",
    "We will need the GPT-4-Turbo and GPT-4-Vision models for this notebook.\n",
    "\n",
    "When running the below cell, the values should reflect the OpenAI reource you have created in the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {\n",
    "        'AZURE_OPENAI_RESOURCE': os.environ.get('AZURE_OPENAI_RESOURCE'),\n",
    "        'AZURE_OPENAI_KEY': os.environ.get('AZURE_OPENAI_KEY'),\n",
    "        'AZURE_OPENAI_MODEL_VISION': os.environ.get('AZURE_OPENAI_MODEL_VISION'),\n",
    "        'AZURE_OPENAI_MODEL': os.environ.get('AZURE_OPENAI_MODEL'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Excel Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"sample_data/11_work_orders.xlsx\"\n",
    "dataframes = read_excel_to_dataframes(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions\n",
      "WOs\n",
      "AdminLists\n",
      "MyLinks\n"
     ]
    }
   ],
   "source": [
    "# Printing the sheet names\n",
    "for k in dataframes: print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|    | 2                                             |\n",
       "|---:|:----------------------------------------------|\n",
       "|  4 | Downloaded From                               |\n",
       "|  5 | Sample Data for Excel                         |\n",
       "|  7 | Related tutorials                             |\n",
       "|  8 | Named Excel Tables                            |\n",
       "|  9 | Data Entry Tips                               |\n",
       "| 10 | More Excel Sample Files                       |\n",
       "| 12 | Notes                                         |\n",
       "| 13 | Fake work order data to use for Excel testing |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nb_table_df_cleanup_df(df):\n",
    "    dfc = copy.deepcopy(df)\n",
    "    dfc = dfc.dropna(axis=0, how='all')\n",
    "    dfc = dfc.dropna(axis=1, how='all')\n",
    "    dfc = dfc.replace(r'\\n','   //    ', regex=True) \n",
    "    dfc = dfc.replace(r'\\|','   ///    ', regex=True) \n",
    "    return dfc\n",
    "\n",
    "\n",
    "df = nb_table_df_cleanup_df(dataframes['Instructions'])\n",
    "md_table = df.to_markdown()\n",
    "Markdown(md_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "columns": "Index, Content",
       "columns_inferred": true,
       "summary_of_the_table": "The table appears to be a list of resources or information related to Excel. It includes items such as a source for downloading data, sample data sets for Excel, tutorials, tips for data entry, and additional sample files. There is also a note about fake work order data for Excel testing. This table could be used as a reference for someone learning Excel or looking for Excel-related resources.",
       "total_number_of_columns": 2
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "markdown_extract_header_and_summarize_prompt = \"\"\"\n",
    "You are a Data Engineer resonsible for reforming and preserving the quality of Markdown tables. A table will be passed to you in the form of a Markdown string. You are designed to output JSON. \n",
    "\n",
    "Your task is to extract the column names of the header of the table from the Markdown string in the form of a comma-separated list. If the column names do exist, please return them verbatim word-for-word with no change, except fixing format or alignment issues (extra spaces and new lines can be removed). \n",
    "\n",
    "If the table does not have a header, then please check the data rows and generate column names for the header that fit the data types of the columns and the nature of the data. \n",
    "\n",
    "**VERY IMPORTANT**: If the table has an unnamed index column, typically the leftmost column, you **MUST** generate a column name for it.\n",
    "\n",
    "Finally, please generate a brief semantic summary of the table in English. This is not about the technical characteristics of the table. The summary should summarize the business purpose and contents of the table. The summary should be to the point with two or three paragraphs.\n",
    "\n",
    "The Markdown table: \n",
    "## START OF MARKDOWN TABLE\n",
    "{table}\n",
    "## END OF MARKDOWN TABLE\n",
    "\n",
    "JSON OUTPUT:\n",
    "You **MUST** generate the below JSON dictionary as your output. \n",
    "\n",
    "{{\n",
    "    \"columns\": \"list of comma-separated column names. If the table has a header, please return the column names as they are. If the table does not have a header, then generate column names that fit the data types and nature of the data. Do **NOT** forget any unnamed index columns.\",\n",
    "    \"columns_inferred\": \"true/false. Set to true in the case the table does not have a header, and you generated column names based on the data rows.\",\n",
    "    \"total_number_of_columns\": \"total number of columns in the table\",\n",
    "    \"summary_of_the_table\": \"a brief semantic summary of the table in English. This is not about the technical characteristics of the table. The summary should summarize the business purpose and contents of the table. The summary should be concise and to the point, one or two short paragraphs.\"\n",
    "}}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = markdown_extract_header_and_summarize_prompt.format(table=md_table)\n",
    "output = ask_LLM_with_JSON(prompt, model_info = model_info)\n",
    "display(JSON(json.loads(output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|    | 0      | 1        | 2        | 3       | 4    | 5                   | 6                   | 7     | 8      | 9        | 10     | 11        | 12      | 13   | 14      | 15      | 16     | 17       | 18                 | 19                 | 20     | 21      |\n",
       "|---:|:-------|:---------|:---------|:--------|:-----|:--------------------|:--------------------|:------|:-------|:---------|:-------|:----------|:--------|:-----|:--------|:--------|:-------|:---------|:-------------------|:-------------------|:-------|:--------|\n",
       "|  0 | WO     | District | LeadTech | Service | Rush | ReqDate             | WorkDate            | Techs | WtyLbr | WtyParts | LbrHrs | PartsCost | Payment | Wait | LbrRate | LbrCost | LbrFee | PartsFee | TotalCost          | TotalFee           | ReqDay | WorkDay |\n",
       "|  1 | A00100 | North    | Khan     | Assess  | nan  | 2020-09-01 00:00:00 | 2020-09-15 00:00:00 | 2     | nan    | nan      | 0.5    | 360       | Account | 14   | 140     | 70      | 70     | 360      | 430                | 430                | Tue    | Tue     |\n",
       "|  2 | A00101 | South    | Lopez    | Replace | nan  | 2020-09-01 00:00:00 | 2020-09-04 00:00:00 | 1     | nan    | nan      | 0.5    | 90.0416   | Account | 3    | 80      | 40      | 40     | 90.0416  | 130.04160000000002 | 130.04160000000002 | Tue    | Fri     |\n",
       "|  3 | A00102 | Central  | Cartier  | Deliver | nan  | 2020-09-01 00:00:00 | 2020-09-17 00:00:00 | 1     | nan    | nan      | 0.25   | 120       | P.O.    | 16   | 80      | 20      | 20     | 120      | 140                | 140                | Tue    | Thu     |\n",
       "|  4 | A00103 | South    | Lopez    | Deliver | nan  | 2020-09-01 00:00:00 | 2020-09-17 00:00:00 | 1     | nan    | nan      | 0.25   | 16.25     | Account | 16   | 80      | 20      | 20     | 16.25    | 36.25              | 36.25              | Tue    | Thu     |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wos = nb_table_df_cleanup_df(dataframes['WOs'])\n",
    "md_table_wos_head = df_wos.head().to_markdown()\n",
    "md_table_wos = df_wos.to_markdown()\n",
    "Markdown(md_table_wos_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Token Count: 113325\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "columns": "Index, WO, District, LeadTech, Service, Rush, ReqDate, WorkDate, Techs, WtyLbr, WtyParts, LbrHrs, PartsCost, Payment, Wait, LbrRate, LbrCost, LbrFee, PartsFee, TotalCost, TotalFee, ReqDay, WorkDay",
       "columns_inferred": "false",
       "summary_of_the_table": "The table appears to be a log of work orders (WO) for technical services provided across various districts. Each entry includes details about the lead technician (LeadTech), the type of service performed, whether the job was rushed, the requested and actual work dates, the number of technicians (Techs) involved, warranty labor and parts information, labor hours (LbrHrs), parts cost, payment method, waiting time (Wait), labor rate (LbrRate), labor cost (LbrCost), labor fee (LbrFee), parts fee, total cost, total fee, and the days of the week for request and work completion. This table is likely used for managing and tracking technical service operations, including financial aspects and scheduling.",
       "total_number_of_columns": "23"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Table Token Count: {get_token_count(md_table_wos)}\")\n",
    "\n",
    "prompt = markdown_extract_header_and_summarize_prompt.format(table=md_table_wos)\n",
    "output = ask_LLM_with_JSON(prompt, model_info = model_info)\n",
    "display(JSON(json.loads(output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Token Count: 113325\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "columns": "Index, WO, District, LeadTech, Service, Rush, ReqDate, WorkDate, Techs, WtyLbr, WtyParts, LbrHrs, PartsCost, Payment, Wait, LbrRate, LbrCost, LbrFee, PartsFee, TotalCost, TotalFee, ReqDay, WorkDay",
       "columns_inferred": false,
       "summary_of_the_table": "The table appears to be a log of work orders (WO) for a service company. It includes details such as the district where the service was performed, the lead technician's name, the type of service provided, whether the job was rushed, the requested and actual work dates, the number of technicians (Techs) involved, warranty information for labor and parts, labor hours (LbrHrs), costs associated with parts and labor, payment method, wait time, labor rates, fees, and the total costs and fees. The table also includes the day of the week for both the request and work completion. This data could be used for managing operations, financial accounting, and analyzing service efficiency and costs.",
       "total_number_of_columns": 23
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Table Token Count: {get_token_count(md_table_wos)}\")\n",
    "\n",
    "prompt = markdown_extract_header_and_summarize_prompt.format(table=md_table_wos.split('\\n')[:100])\n",
    "output = ask_LLM_with_JSON(prompt, model_info = model_info)\n",
    "display(JSON(json.loads(output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 319\n",
      "The table appears to be a log of work orders (WO) for a service company. Each row represents a job with details such as the district where the job took place, the lead technician (LeadTech), the type of service performed, whether it was a rush job, request and work dates, the number of technicians (Techs) involved, warranty labor and parts information, labor hours (LbrHrs), parts cost, payment method, waiting time (Wait), labor rate (LbrRate), labor cost (LbrCost), labor fee (LbrFee), parts fee (PartsFee), total cost (TotalCost), total fee (TotalFee), and the days of the week when the request was made and the work was done (ReqDay and WorkDay). This table is useful for tracking job costs, technician assignments, and scheduling efficiency.\n"
     ]
    }
   ],
   "source": [
    "def chunk_markdown_table_with_overlap(md_table, cols = None, n_tokens = 512, overlap = 128):\n",
    "\n",
    "    mds = md_table.split('\\n')\n",
    "\n",
    "    if cols is not None:\n",
    "        header = '|   ' + '   |   '.join(cols) + '   |\\n'\n",
    "    else:\n",
    "        header = mds[0] + '\\n'\n",
    "\n",
    "    chunks = []\n",
    "    chunk = header\n",
    "\n",
    "    for i, r in enumerate(mds[1:]):\n",
    "        chunk += r + '\\n'\n",
    "\n",
    "        ## Check if the chunk is over n_tokens\n",
    "        if get_token_count(chunk) > n_tokens:\n",
    "            ## Add Overlap\n",
    "            try:\n",
    "                for j, ovr in enumerate(mds[i + 1:]):\n",
    "                    chunk += ovr + '\\n'\n",
    "                    if get_token_count(chunk) > n_tokens + overlap:\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            \n",
    "            chunks.append(chunk)        \n",
    "\n",
    "            # print(f\"Chunk {len(chunks)}: {get_token_count(chunk)}\")\n",
    "            chunk = header  + mds[1] + '\\n'\n",
    "\n",
    "    return chunks, header\n",
    "\n",
    "\n",
    "def chunk_markdown_table(df, model_info):\n",
    "\n",
    "    df_clean = nb_table_df_cleanup_df(df)\n",
    "    md_table = df_clean.to_markdown()\n",
    "\n",
    "    prompt = markdown_extract_header_and_summarize_prompt.format(table=md_table.split('\\n')[:100])\n",
    "    output = ask_LLM_with_JSON(prompt, model_info = model_info)\n",
    "    outd = json.loads(output)\n",
    "    cols = outd['columns'].split(',')\n",
    "    summary = outd['summary_of_the_table']\n",
    "\n",
    "    chunks, header = chunk_markdown_table_with_overlap(md_table, cols, n_tokens = 512, overlap = 128)\n",
    "    print(\"Chunks:\", len(chunks))\n",
    "    return chunks, header, summary\n",
    "\n",
    "chunks, header, summary = chunk_markdown_table(dataframes['WOs'], model_info)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|   Index   |    WO   |    District   |    LeadTech   |    Service   |    Rush   |    ReqDate   |    WorkDate   |    Techs   |    WtyLbr   |    WtyParts   |    LbrHrs   |    PartsCost   |    Payment   |    Wait   |    LbrRate   |    LbrCost   |    LbrFee   |    PartsFee   |    TotalCost   |    TotalFee   |    ReqDay   |    WorkDay   |\n",
       "|-----:|:-------|:----------|:---------|:--------|:-----|:--------------------|:--------------------|:------|:-------|:---------|:-------|:----------|:---------|:-----|:--------|:--------|:-------|:----------|:-------------------|:-------------------|:-------|:--------|\n",
       "|    0 | WO     | District  | LeadTech | Service | Rush | ReqDate             | WorkDate            | Techs | WtyLbr | WtyParts | LbrHrs | PartsCost | Payment  | Wait | LbrRate | LbrCost | LbrFee | PartsFee  | TotalCost          | TotalFee           | ReqDay | WorkDay |\n",
       "|    1 | A00100 | North     | Khan     | Assess  | nan  | 2020-09-01 00:00:00 | 2020-09-15 00:00:00 | 2     | nan    | nan      | 0.5    | 360       | Account  | 14   | 140     | 70      | 70     | 360       | 430                | 430                | Tue    | Tue     |\n",
       "|    2 | A00101 | South     | Lopez    | Replace | nan  | 2020-09-01 00:00:00 | 2020-09-04 00:00:00 | 1     | nan    | nan      | 0.5    | 90.0416   | Account  | 3    | 80      | 40      | 40     | 90.0416   | 130.04160000000002 | 130.04160000000002 | Tue    | Fri     |\n",
       "|    3 | A00102 | Central   | Cartier  | Deliver | nan  | 2020-09-01 00:00:00 | 2020-09-17 00:00:00 | 1     | nan    | nan      | 0.25   | 120       | P.O.     | 16   | 80      | 20      | 20     | 120       | 140                | 140                | Tue    | Thu     |\n",
       "|    3 | A00102 | Central   | Cartier  | Deliver | nan  | 2020-09-01 00:00:00 | 2020-09-17 00:00:00 | 1     | nan    | nan      | 0.25   | 120       | P.O.     | 16   | 80      | 20      | 20     | 120       | 140                | 140                | Tue    | Thu     |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|   Index   |    WO   |    District   |    LeadTech   |    Service   |    Rush   |    ReqDate   |    WorkDate   |    Techs   |    WtyLbr   |    WtyParts   |    LbrHrs   |    PartsCost   |    Payment   |    Wait   |    LbrRate   |    LbrCost   |    LbrFee   |    PartsFee   |    TotalCost   |    TotalFee   |    ReqDay   |    WorkDay   |\n",
       "|-----:|:-------|:----------|:---------|:--------|:-----|:--------------------|:--------------------|:------|:-------|:---------|:-------|:----------|:---------|:-----|:--------|:--------|:-------|:----------|:-------------------|:-------------------|:-------|:--------|\n",
       "|  927 | A01026 | West      | Khan     | Deliver | nan  | 2021-07-02 00:00:00 | nan                 | 1     | nan    | nan      | nan    | 74.7804   | Account  | nan  | 80      | 0       | 0      | 74.7804   | 74.7804            | 74.7804            | Fri    | Sat     |\n",
       "|  928 | A01027 | Central   | Cartier  | Install | nan  | 2021-07-02 00:00:00 | nan                 | 2     | nan    | nan      | nan    | 445.1606  | C.O.D.   | nan  | 140     | 0       | 0      | 445.1606  | 445.1606           | 445.1606           | Fri    | Sat     |\n",
       "|  929 | A01028 | Central   | Khan     | Assess  | nan  | 2021-07-05 00:00:00 | 2021-07-20 00:00:00 | 2     | nan    | nan      | 0.5    | 85.32     | Account  | 15   | 140     | 70      | 70     | 85.32     | 155.32             | 155.32             | Mon    | Tue     |\n",
       "|  930 | A01029 | West      | Khan     | Assess  | nan  | 2021-07-05 00:00:00 | nan                 | 2     | nan    | nan      | nan    | 180.33    | Account  | nan  | 140     | 0       | 0      | 180.33    | 180.33             | 180.33             | Mon    | Sat     |\n",
       "|  930 | A01029 | West      | Khan     | Assess  | nan  | 2021-07-05 00:00:00 | nan                 | 2     | nan    | nan      | nan    | 180.33    | Account  | nan  | 140     | 0       | 0      | 180.33    | 180.33             | 180.33             | Mon    | Sat     |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(chunks[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
