{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping and Ingestion\n",
    "\n",
    "\n",
    "This notebook explains how to scrape data from a base url with a specified depth when navigating the root url.\n",
    "\n",
    "`depth` specifies the level of exploitation to the root url, this is necessary to avoid infinite loops when interacting with urls that have a lot of references (ex. Wikipedia)\n",
    "\n",
    "`depth = 2` Crawler will crawl the sublinks of the root link along with all the sublinks of the sublinks of the root link, then stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (1.16.2)\n",
      "Collecting openai\n",
      "  Using cached openai-1.20.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: tiktoken in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (0.6.0)\n",
      "Requirement already satisfied: langchain in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (0.1.14)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.1.16-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from openai) (2.6.4)\n",
      "Requirement already satisfied: sniffio in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from langchain) (0.5.14)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from langchain) (1.33)\n",
      "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
      "  Downloading langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
      "  Downloading langchain_core-0.1.43-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from langchain) (0.1.41)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ali/anaconda3/envs/mmdoc/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Using cached openai-1.20.0-py3-none-any.whl (292 kB)\n",
      "Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.0.33-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.1.43-py3-none-any.whl (289 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.1/289.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openai, langchain-core, langchain-community, langchain\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.16.2\n",
      "    Uninstalling openai-1.16.2:\n",
      "      Successfully uninstalled openai-1.16.2\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.40\n",
      "    Uninstalling langchain-core-0.1.40:\n",
      "      Successfully uninstalled langchain-core-0.1.40\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.0.31\n",
      "    Uninstalling langchain-community-0.0.31:\n",
      "      Successfully uninstalled langchain-community-0.0.31\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.1.14\n",
      "    Uninstalling langchain-0.1.14:\n",
      "      Successfully uninstalled langchain-0.1.14\n",
      "Successfully installed langchain-0.1.16 langchain-community-0.0.33 langchain-core-0.1.43 openai-1.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "%pip install --upgrade openai beautifulsoup4 requests tiktoken langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def extract_html_and_media_urls(url):\n",
    "    \"\"\"Extracts HTML content and media (images and videos) URLs from a given URL.\"\"\"\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises an HTTPError if the status is 4xx, 5xx\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract HTML\n",
    "        html_content = soup.prettify()\n",
    "\n",
    "        # Extract image URLs\n",
    "        images = [urljoin(url, img['src']) for img in soup.find_all('img') if 'src' in img.attrs]\n",
    "\n",
    "        # Extract video URLs\n",
    "        videos = [urljoin(url, video['src']) for video in soup.find_all('video') if 'src' in video.attrs]\n",
    "\n",
    "        # Optionally, you can print or return the HTML content, images, and videos\n",
    "        return html_content, images, videos\n",
    "    except requests.RequestException as e:\n",
    "        return f\"An error occurred: {e}\", [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "url = \"https://example.com/\"\n",
    "html_content, images, videos = extract_html_and_media_urls(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Number of Tokens: 376\n",
      "Number of Image URLs: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"HTML Number of Tokens:\", num_tokens_from_string(html_content, \"cl100k_base\"))\n",
    "print(\"Number of Image URLs:\", len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure we have the OpenAI Models information\n",
    "\n",
    "We will need the GPT-4-Turbo and GPT-4-Vision models for this notebook.\n",
    "\n",
    "When running the below cell, the values should reflect the OpenAI reource you have created in the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_API_BASE = os.getenv(\"AZURE_OPENAI_RESOURCE\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_client = AzureOpenAI(\n",
    "    azure_endpoint = AZURE_OPENAI_API_BASE, \n",
    "    api_key= AZURE_OPENAI_API_KEY,  \n",
    "    api_version= AZURE_OPENAI_API_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = \"gpt-4\" # Fill in the deployment name from the portal here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_gpt_cleaning(html_content: str) -> str:\n",
    "    system_prompt = \"\"\"\n",
    "    You are an efficient web scraper. Your goal is to take large HTML files and clean them up. The focus should be on reducing all the redundant HTML tags but keep the overall structure the same.\n",
    "    Some of the things you should do are:\n",
    "    - Remove all the redundant tags.\n",
    "    - Extract the content from the tags and keep it in the same order.\n",
    "    - The output should be in markdown format\n",
    "    \"\"\"\n",
    "\n",
    "    response = oai_client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": html_content},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        \n",
    "    )\n",
    "    print(f\"{response.choices[0].message.role}: {response.choices[0].message.content}\")\n",
    "    return response.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_content(url_string, html_content, markdown_content):\n",
    "    \"\"\"\n",
    "    Saves HTML and Markdown content into separate files within a directory named after the given URL.\n",
    "\n",
    "    Parameters:\n",
    "    - url_string: The URL string used to name the directory.\n",
    "    - html_content: The HTML content to be saved.\n",
    "    - markdown_content: The Markdown content to be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to sanitize and create a directory name from the URL\n",
    "    def create_directory_from_url(url):\n",
    "        # Remove protocol (http, https) and replace forbidden characters\n",
    "        for protocol in ['http://', 'https://']:\n",
    "            url = url.replace(protocol, '')\n",
    "        # Replace slashes and other forbidden characters with underscores\n",
    "        forbidden_chars = ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|']\n",
    "        for char in forbidden_chars:\n",
    "            url = url.replace(char, '_')\n",
    "        return url\n",
    "\n",
    "    # Create directory path from URL\n",
    "    directory_path = os.path.join('data', create_directory_from_url(url_string))\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    # Save HTML content to a file\n",
    "    html_file_path = os.path.join(directory_path, 'content.html')\n",
    "    with open(html_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    # Save Markdown content to a file\n",
    "    markdown_file_path = os.path.join(directory_path, 'content.md')\n",
    "    with open(markdown_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(markdown_content)\n",
    "\n",
    "    print(f\"Files saved in {directory_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved in data/example.com_article\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "save_content(\"http://example.com/article\", \"<html>Your HTML content here</html>\", \"Your Markdown content here\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "\n",
    "\n",
    "def is_valid_url(url):\n",
    "    \"\"\"Check if a URL is valid and not an internal link.\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "def get_links(url, session):\n",
    "    \"\"\"Return all valid hyperlinks found on the specified webpage, handle PDF and HTML content.\"\"\"\n",
    "    try:\n",
    "        response = session.get(url, timeout=5)\n",
    "        response.raise_for_status()  # Ensure we notice bad responses\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = [urljoin(url, link.get('href')) for link in soup.find_all('a', href=True)]\n",
    "        return set(filter(is_valid_url, links))\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return set()\n",
    "\n",
    "\n",
    "def scrape_site(root_url, max_depth=3, max_links=100):\n",
    "    \"\"\"Scrape a site up to a maximum depth from the root URL or until max_links have been visited, downloading PDFs and HTML encountered.\"\"\"\n",
    "    session = requests.Session()\n",
    "    visited = set()\n",
    "    \n",
    "    def _scrape(url, depth):\n",
    "        if url in visited or depth > max_depth or len(visited) >= max_links:\n",
    "            return\n",
    "        visited.add(url)\n",
    "        print(f\"Visiting: {url} | Depth: {depth} | Total Visited: {len(visited)}\")\n",
    "        print(f\"Processing: {url}\")\n",
    "        html_content, _, _ = extract_html_and_media_urls(url)\n",
    "        processed_content = run_gpt_cleaning(html_content)\n",
    "        save_content(url, html_content, processed_content)\n",
    "        \n",
    "        links = get_links(url, session)\n",
    "        for link in links:\n",
    "            _scrape(link, depth + 1)\n",
    "    \n",
    "    _scrape(root_url, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting: https://example.com/ | Depth: 0 | Total Visited: 1\n",
      "Processing: https://example.com/\n",
      "assistant: # Example Domain\n",
      "\n",
      "This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for permission.\n",
      "\n",
      "[More information...](https://www.iana.org/domains/example)\n",
      "Files saved in data/example.com_\n",
      "Visiting: https://www.iana.org/domains/example | Depth: 1 | Total Visited: 2\n",
      "Processing: https://www.iana.org/domains/example\n",
      "assistant: # Example Domains\n",
      "\n",
      "As described in [RFC 2606](/go/rfc2606) and [RFC 6761](/go/rfc6761), a number of domains such as example.com and example.org are maintained for documentation purposes. These domains may be used as illustrative examples in documents without prior coordination with us. They are not available for registration or transfer.\n",
      "\n",
      "We provide a web service on the example domain hosts to provide basic information on the purpose of the domain. These web services are provided as best effort, but are not designed to support production applications. While incidental traffic for incorrectly configured applications is expected, please do not design applications that require the example domains to have operating HTTP service.\n",
      "\n",
      "## Further Reading\n",
      "\n",
      "- [IANA-managed Reserved Domains](/domains/reserved)\n",
      "\n",
      "Last revised 2017-05-13.\n",
      "\n",
      "---\n",
      "\n",
      "- [Domain Names](/domains)\n",
      "  - [Root Zone Registry](/domains/root)\n",
      "  - [.INT Registry](/domains/int)\n",
      "  - [.ARPA Registry](/domains/arpa)\n",
      "  - [IDN Repository](/domains/idn-tables)\n",
      "- [Number Resources](/numbers)\n",
      "  - [Abuse Information](/abuse)\n",
      "- [Protocols](/protocols)\n",
      "  - [Protocol Registries](/protocols)\n",
      "  - [Time Zone Database](/time-zones)\n",
      "- [About Us](/about)\n",
      "  - [Performance](/performance)\n",
      "  - [Reports](/reports)\n",
      "  - [Reviews](/reviews)\n",
      "  - [Excellence](/about/excellence)\n",
      "  - [Contact Us](/contact)\n",
      "\n",
      "The IANA functions coordinate the Internet’s globally unique identifiers, and are provided by [Public Technical Identifiers](http://pti.icann.org), an affiliate of [ICANN](http://www.icann.org/).\n",
      "\n",
      "- [Privacy Policy](https://www.icann.org/privacy/policy)\n",
      "- [Terms of Service](https://www.icann.org/privacy/tos)\n",
      "Files saved in data/www.iana.org_domains_example\n",
      "Visiting: https://www.iana.org/about | Depth: 2 | Total Visited: 3\n",
      "Processing: https://www.iana.org/about\n",
      "assistant: # About us\n",
      "\n",
      "We are responsible for coordinating some of the key elements that keep the Internet running smoothly. Whilst the Internet is renowned for being a worldwide network free from central coordination, there is a technical need for some key parts of the Internet to be globally coordinated, and this coordination role is undertaken by us.\n",
      "\n",
      "Specifically, we allocate and maintain unique codes and numbering systems that are used in the technical standards (“protocols”) that drive the Internet.\n",
      "\n",
      "Our various activities can be broadly grouped in to three categories:\n",
      "\n",
      "- [Domain Names](/domains/)\n",
      "  Management of the DNS Root, the .int and .arpa domains, and an IDN practices resource.\n",
      "\n",
      "- [Number Resources](/numbers/)\n",
      "  Co-ordination of the global pool of IP and AS numbers, primarily providing them to Regional Internet Registries (RIRs).\n",
      "\n",
      "- [Protocol Assignments](/protocols/)\n",
      "  Internet protocols’ numbering systems are managed in conjunction with standards bodies.\n",
      "\n",
      "We are one of the Internet's oldest institutions, with the IANA functions dating back to the 1970s. Today the services are provided by [Public Technical Identifiers](http://pti.icann.org/), a purpose-built organization for providing the IANA functions to the community. PTI is an affiliate of [ICANN](http://www.icann.org), an internationally-organised non-profit organisation set up by the Internet community to coordinate our areas of responsibilities.\n",
      "\n",
      "## Mission Statement\n",
      "\n",
      "This statement describes the role of PTI:\n",
      "\n",
      "*PTI is responsible for the operational aspects of coordinating the Internet’s unique identifiers and maintaining the trust of the community to provide these services in an unbiased, responsible and effective manner.*\n",
      "\n",
      "## Our Policy Remit\n",
      "\n",
      "We do not directly set policy by which we operate, instead we implement agreed policies and principles in a neutral and responsible manner. Using the policy-setting forums provided by ICANN, policy development for domain name operations and IP addressing is arrived at by many different stakeholders. ICANN has a structure of supporting organisations that contribute to deciding how ICANN runs, which in turn informs how PTI is operated. The development of Internet protocols, which often dictate how protocol assignments should be managed, are arrived at within the Internet Engineering Task Force, the Internet Engineering Steering Group, and the Internet Architecture Board.\n",
      "\n",
      "To improve its operations, we are actively involved in outreach too. As well as in ICANN forums, we participate in meetings and discussions with TLD operators, Regional Internet Registries, and other relevant communities. We provide manned helpdesks at key meetings to allow one-to-one interaction with our community of users, such as protocol developers and operators of critical Internet infrastructure.\n",
      "\n",
      "---\n",
      "\n",
      "### About us\n",
      "\n",
      "- [Introduction](/about)\n",
      "- [Performance Reporting](/performance)\n",
      "- [Procedures](/procedures)\n",
      "- [Presentations](/about/presentations)\n",
      "- [Public Reports](/reports)\n",
      "- [Framework Documents](/about/framework)\n",
      "- [Reviews](/reviews)\n",
      "- [Audits](/about/audits)\n",
      "- [Excellence & Quality](/about/excellence)\n",
      "- [Contact us](/contact)\n",
      "\n",
      "---\n",
      "\n",
      "### Domain Names\n",
      "\n",
      "- [Root Zone Registry](/domains/root)\n",
      "- [.INT Registry](/domains/int)\n",
      "- [.ARPA Registry](/domains/arpa)\n",
      "- [IDN Repository](/domains/idn-tables)\n",
      "\n",
      "### Number Resources\n",
      "\n",
      "- [Abuse Information](/abuse)\n",
      "\n",
      "### Protocols\n",
      "\n",
      "- [Protocol Registries](/protocols)\n",
      "- [Time Zone Database](/time-zones)\n",
      "\n",
      "### About Us\n",
      "\n",
      "- [Performance](/performance)\n",
      "- [Reports](/reports)\n",
      "- [Reviews](/reviews)\n",
      "- [Excellence](/about/excellence)\n",
      "- [Contact Us](/contact)\n",
      "\n",
      "The IANA functions coordinate the Internet’s globally unique identifiers, and are provided by [Public Technical Identifiers](http://pti.icann.org), an affiliate of [ICANN](http://www.icann.org/).\n",
      "\n",
      "- [Privacy Policy](https://www.icann.org/privacy/policy)\n",
      "- [Terms of Service](https://www.icann.org/privacy/tos)\n",
      "Files saved in data/www.iana.org_about\n"
     ]
    }
   ],
   "source": [
    "scrape_site('https://example.com/', max_depth=3, max_links=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "oai_emb_client = AzureOpenAI(\n",
    "    azure_endpoint = AZURE_OPENAI_API_BASE, \n",
    "    api_key= AZURE_OPENAI_API_KEY,\n",
    "    api_version= AZURE_OPENAI_API_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup AI Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing AI Search Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_SEARCH_ENDPOINT = os.environ.get('COG_SEARCH_ENDPOINT')\n",
    "AI_SEARCH_ADMIN_KEY = os.environ.get('COG_SEARCH_ADMIN_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.cogsearch_rest import *\n",
    "\n",
    "index_name = 'wikipedia_resources'\n",
    "\n",
    "fields = [\n",
    "            {\"name\": \"id\", \"type\": \"Edm.String\", \"key\": True, \"searchable\": True, \"filterable\": True, \"retrievable\": True, \"sortable\": True},\n",
    "            {\"name\": \"vector\", \"type\": \"Collection(Edm.Single)\", \"searchable\": True,\"retrievable\": True, \"dimensions\": 1536,\"vectorSearchProfile\": \"my-vector-profile\"},\n",
    "            {\"name\": \"tags\", \"type\": \"Edm.String\",\"searchable\": True, \"filterable\": False, \"retrievable\": True, \"sortable\": False, \"facetable\": False},\n",
    "            {\"name\": \"text\", \"type\": \"Edm.String\",\"searchable\": True, \"filterable\": False, \"retrievable\": True, \"sortable\": False, \"facetable\": False},\n",
    "]\n",
    "\n",
    "index = CogSearchRestAPI(index_name, fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True) # take environment variables from .env.\n",
    "\n",
    "# Variables not used here do not need to be updated in your .env file\n",
    "endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "key_credential = os.environ[\"AZURE_SEARCH_ADMIN_KEY\"] if len(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]) > 0 else None\n",
    "index_name = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "azure_openai_key = os.environ[\"AZURE_OPENAI_KEY\"] if len(os.environ[\"AZURE_OPENAI_KEY\"]) > 0 else None\n",
    "azure_openai_embedding_deployment = os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"]\n",
    "embedding_model_name = os.environ[\"AZURE_OPENAI_EMBEDDING_MODEL_NAME\"]\n",
    "azure_openai_api_version = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n",
    "credential = key_credential or DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "import json\n",
    "\n",
    "openai_credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(openai_credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_deployment=azure_openai_embedding_deployment,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_key,\n",
    "    azure_ad_token_provider=token_provider if not azure_openai_key else None\n",
    ")\n",
    "\n",
    "\n",
    "titles = [item['title'] for item in input_data]\n",
    "content = [item['content'] for item in input_data]\n",
    "title_response = client.embeddings.create(input=titles, model=embedding_model_name)\n",
    "title_embeddings = [item.embedding for item in title_response.data]\n",
    "content_response = client.embeddings.create(input=content, model=embedding_model_name)\n",
    "content_embeddings = [item.embedding for item in content_response.data]\n",
    "\n",
    "# Generate embeddings for title and content fields\n",
    "for i, item in enumerate(input_data):\n",
    "    title = item['title']\n",
    "    content = item['content']\n",
    "    item['titleVector'] = title_embeddings[i]\n",
    "    item['contentVector'] = content_embeddings[i]\n",
    "\n",
    "# Output embeddings to docVectors.json file\n",
    "output_path = os.path.join('..', 'output', 'docVectors.json')\n",
    "output_directory = os.path.dirname(output_path)\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(input_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchFieldDataType,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    SearchIndex\n",
    ")\n",
    "\n",
    "\n",
    "# Create a search index\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=endpoint, credential=AzureKeyCredential(credential))\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"url\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"titleVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),\n",
    "    SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),\n",
    "]\n",
    "\n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\"\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        content_fields=[SemanticField(field_name=\"content\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields,\n",
    "                    vector_search=vector_search, semantic_search=semantic_search)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Upload some documents to the index\n",
    "output_path = os.path.join('..', 'output', 'docVectors.json')\n",
    "output_directory = os.path.dirname(output_path)\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "with open(output_path, 'r') as file:  \n",
    "    documents = json.load(file)  \n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=AzureKeyCredential(credential))\n",
    "result = search_client.upload_documents(documents)\n",
    "print(f\"Uploaded {len(documents)} documents\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "# Pure Vector Search\n",
    "# query = \"Where does the word Mathematics come from?\"  \n",
    "query = \"When was the first international football match played?\"  \n",
    "  \n",
    "embedding = client.embeddings.create(input=query, model=embedding_model_name).data[0].embedding\n",
    "vector_query = VectorizedQuery(vector=embedding, k_nearest_neighbors=3, fields=\"contentVector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"title\", \"content\", \"url\"],\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"URL: {result['url']}\\n\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
