{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Audio Files\n",
    "\n",
    "Read WAV or MP3 Files with Python and transcribe to text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-cognitiveservices-speech in c:\\users\\angels\\appdata\\local\\miniconda3\\lib\\site-packages (1.30.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openai in c:\\users\\angels\\appdata\\local\\miniconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in c:\\users\\angels\\appdata\\local\\miniconda3\\lib\\site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\angels\\appdata\\local\\miniconda3\\lib\\site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\angels\\appdata\\local\\miniconda3\\lib\\site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\angels\\appdata\\local\\miniconda3\\lib\\site-packages (from openai) (2.5.3)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\angels\\appdata\\local\\miniconda3\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in c:\\users\\angels\\appdata\\local\\miniconda3\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\angels\\appdata\\local\\miniconda3\\lib\\site-packages (from anyio<4,>=3.5.0->openai) (2.8)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\angels\\appdata\\local\\miniconda3\\lib\\site-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\angels\\appdata\\local\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2019.11.28)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\angels\\appdata\\local\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\angels\\appdata\\local\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\angels\\appdata\\local\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\angels\\appdata\\local\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\angels\\appdata\\local\\miniconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-cognitiveservices-speech\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import azure.cognitiveservices.speech as speech_sdk\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "sys.path.append('..\\\\code')\n",
    "\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure we have the Azure Speech information\n",
    "\n",
    "We will need the Speech APIKEY, REGION and LANGUAGE for this notebook.\n",
    "\n",
    "When running the below cell, the values should reflect the Azure Speech reource you have created in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SPEECH_APIKEY': '6e067df5f9ff46cfac9cda6f8f122c60',\n",
       " 'SPEECH_REGION': 'westeurope',\n",
       " 'SPEECH_LANGUAGE': 'en-US'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_info = {\n",
    "        'SPEECH_APIKEY': os.environ.get('SPEECH_APIKEY'),\n",
    "        'SPEECH_REGION': os.environ.get('SPEECH_REGION'),\n",
    "        'SPEECH_LANGUAGE': os.environ.get('SPEECH_LANGUAGE'),\n",
    "}\n",
    "\n",
    "speech_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AZURE_OPENAI_MODEL_WHISPER': 'whisper',\n",
       " 'AZURE_OPENAI_KEY': 'daf52e67bb574e18ac4467cd6f787c83',\n",
       " 'AZURE_OPENAI_ENDPOINT_WHISPER': 'https://openai-angels.openai.azure.com/',\n",
       " 'AZURE_OPENAI_VERSION_WHISPER': '2024-02-01'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info = {\n",
    "        'AZURE_OPENAI_MODEL_WHISPER': os.environ.get('AZURE_OPENAI_MODEL_WHISPER'),\n",
    "        'AZURE_OPENAI_KEY': os.environ.get('AZURE_OPENAI_KEY'),\n",
    "        'AZURE_OPENAI_MODEL_WHISPER': os.environ.get('AZURE_OPENAI_MODEL_WHISPER'),\n",
    "        'AZURE_OPENAI_ENDPOINT_WHISPER': os.environ.get('AZURE_OPENAI_ENDPOINT_WHISPER'),\n",
    "        'AZURE_OPENAI_VERSION_WHISPER': os.environ.get('AZURE_OPENAI_VERSION_WHISPER'),\n",
    "}\n",
    "\n",
    "model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Definitions\n",
    "\n",
    "Defining the functions that will read in the audio file and return the transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Azure Speech Service\n",
    "def config_speech_service():\n",
    "    try:\n",
    "        speech_config = speech_sdk.SpeechConfig(\n",
    "            subscription=speech_info['SPEECH_APIKEY'], \n",
    "            region=speech_info['SPEECH_REGION'], \n",
    "            speech_recognition_language=speech_info['SPEECH_LANGUAGE'])\n",
    "\n",
    "        # Set parameters\n",
    "        speech_config.set_property(speech_sdk.PropertyId.SpeechServiceConnection_InitialSilenceTimeoutMs, \"5000\")\n",
    "        speech_config.set_property(speech_sdk.PropertyId.Speech_SegmentationSilenceTimeoutMs, \"2000\")\n",
    "        speech_config.set_property(speech_sdk.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs, \"5000\")\n",
    "    \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "    return speech_config\n",
    "\n",
    "# Execute the transcription from file with Azure Speech service \n",
    "def speech_recognize_continuous_from_file(speech_config, filename):\n",
    "    # Performs continuous speech recognition with input from an audio file\"\"\"\n",
    "    audio_config = speech_sdk.AudioConfig(filename=filename)\n",
    "\n",
    "    speech_recognizer = speech_sdk.SpeechRecognizer(speech_config, audio_config)\n",
    "\n",
    "    done = False\n",
    "    transcription = []\n",
    "\n",
    "    # Callback that signals to stop continuous recognition upon receiving an event `evt`\n",
    "    def stop_cb(evt: speech_sdk.SessionEventArgs):\n",
    "        print('CLOSING')\n",
    "        nonlocal done\n",
    "        done = True\n",
    "    \n",
    "    # Callback that signals the recognition has been canceled\n",
    "    def speech_recognizer_recognition_canceled_cb(evt: speech_sdk.SessionEventArgs):\n",
    "        print('Canceled event')\n",
    "\n",
    "    # Callback that signals the recognition session has been stopped\n",
    "    def speech_recognizer_session_stopped_cb(evt: speech_sdk.SessionEventArgs):\n",
    "        print('SessionStopped event')\n",
    "\n",
    "    # Callback while transcribing\n",
    "    def speech_recognizer_recognizing_cb(evt: speech_sdk.SpeechRecognitionEventArgs):\n",
    "        print('Transcribing: ', evt.result.text)\n",
    "\n",
    "    # Callback when a sentence has finished\n",
    "    def speech_recognizer_transcribed_cb(evt: speech_sdk.SpeechRecognitionEventArgs):\n",
    "        print('TRANSCRIBED:')\n",
    "        if evt.result.reason == speech_sdk.ResultReason.RecognizedSpeech:\n",
    "            print(f'\\tText: {evt.result.text}')\n",
    "            transcription.append(evt.result.text)\n",
    "        elif evt.result.reason == speech_sdk.ResultReason.NoMatch:\n",
    "            print(f'\\tNOMATCH: Speech could not be TRANSCRIBED: {evt.result.no_match_details}')\n",
    "            stop_cb(evt)\n",
    "\n",
    "    # Callback that signal the session has started\n",
    "    def speech_recognizer_session_started_cb(evt: speech_sdk.SessionEventArgs):\n",
    "        print('SessionStarted event')\n",
    "\n",
    "    # Connect callbacks to the events fired by the speech recognizer\n",
    "    speech_recognizer.recognizing.connect(speech_recognizer_recognizing_cb)\n",
    "    speech_recognizer.recognized.connect(speech_recognizer_transcribed_cb)\n",
    "    speech_recognizer.session_started.connect(speech_recognizer_session_started_cb)\n",
    "    speech_recognizer.session_stopped.connect(speech_recognizer_session_stopped_cb)\n",
    "    speech_recognizer.canceled.connect(speech_recognizer_recognition_canceled_cb)\n",
    "    # stop transcribing on either session stopped or canceled events\n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    # Start continuous speech recognition\n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        time.sleep(.5)\n",
    "            \n",
    "    final_text = \"\"\n",
    "    for text in transcription:\n",
    "        final_text += text + \" \\n\"\n",
    "    #print(f'TRANSCRIPTION: [{final_text}]')\n",
    "\n",
    "    speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "    return transcription\n",
    "\n",
    "def config_whisper():\n",
    "    whisper_client = AzureOpenAI(\n",
    "        api_key=model_info['AZURE_OPENAI_KEY'],  \n",
    "        api_version=model_info['AZURE_OPENAI_VERSION_WHISPER'],\n",
    "        base_url=f\"{model_info['AZURE_OPENAI_ENDPOINT_WHISPER']}/openai/deployments/{model_info['AZURE_OPENAI_MODEL_WHISPER']}\"\n",
    "    )\n",
    "\n",
    "    return whisper_client\n",
    "\n",
    "\n",
    "def transcribe_with_whisper(whisper_client, filename):\n",
    "    try:\n",
    "        transcript = whisper_client.audio.transcriptions.create(\n",
    "            file=open(filename, \"rb\"), \n",
    "            model=model_info['AZURE_OPENAI_MODEL_WHISPER']\n",
    "            )\n",
    "        return transcript\n",
    "    \n",
    "    except Exception as ex:\n",
    "        return ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Audio File\n",
    "\n",
    "Read the audio file and print the transcription out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SessionStarted event\n",
      "Transcribing:  the patient report\n",
      "Transcribing:  the patient reported no neurom\n",
      "Transcribing:  the patient reported no neuromuscular\n",
      "Transcribing:  the patient reported no neuromuscular complaints\n",
      "Transcribing:  the patient reported no neuromuscular complaints and on\n",
      "Transcribing:  the patient reported no neuromuscular complaints and on physical\n",
      "Transcribing:  the patient reported no neuromuscular complaints and on physical exam showed\n",
      "Transcribing:  the patient reported no neuromuscular complaints and on physical exam showed no overt mus\n",
      "Transcribing:  the patient reported no neuromuscular complaints and on physical exam showed no overt muscle weakness\n",
      "TRANSCRIBED:\n",
      "\tText: The patient reported no neuromuscular complaints and on physical exam showed no overt muscle weakness.\n",
      "Canceled event\n",
      "CLOSING\n",
      "SessionStopped event\n",
      "CLOSING\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The patient reported no neuromuscular complaints and on physical exam showed no overt muscle weakness.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usage with Azure Speech service\n",
    "speech_config=config_speech_service()\n",
    "#file_path = 'sample_data/sample_audio_parte_accidente.wav'\n",
    "#file_path = 'sample_data/The_National_Park.wav'\n",
    "#file_path = 'sample_data/CNVSample049.wav'\n",
    "file_path = 'call_recording_en.wav'\n",
    "transcript = speech_recognize_continuous_from_file(speech_config, file_path)\n",
    "display(transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileNotFoundError(2, 'No such file or directory')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usage with Whisper\n",
    "whisper_client = config_whisper()\n",
    "#file_path = 'sample_data/CNVSample049.wav'\n",
    "file_path = 'sample_data/call_recording_en.wav'\n",
    "transcript = transcribe_with_whisper(whisper_client, file_path)\n",
    "display(transcript)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
